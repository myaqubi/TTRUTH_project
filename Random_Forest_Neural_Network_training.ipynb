{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675115b9-af11-41a6-ba73-0bd086c8b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the data\n",
    "Moein_New = pd.read_csv('C:/input_file.csv', header=0)\n",
    "Moein_New = Moein_New.transpose()\n",
    "\n",
    "new_header = Moein_New.iloc[0]  # Get the new header from the first row\n",
    "Moein_New = Moein_New[1:]  # Remove the first row from the data\n",
    "Moein_New.columns = new_header  # Set the new header\n",
    "\n",
    "X = Moein_New.drop('Sample', axis=1)  # Features\n",
    "y = Moein_New['Sample']  # Target variable\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "y_labels = pd.factorize(y)[0]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_scaled, y_labels)\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Define neural network architecture\n",
    "def build_model(input_shape):\n",
    "  input_layer = Input(shape=(input_shape,))\n",
    "x = Dense(2600, activation='relu')(input_layer)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(400, activation='relu')(x)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(40, activation='relu')(x)\n",
    "output_layer = Dense(len(np.unique(y_labels)), activation='softmax')(x)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "return model\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Function to train, evaluate and save model and features\n",
    "def train_and_evaluate(X_selected, y_labels, y, feature_count, X_full):\n",
    "  kfold = StratifiedKFold(n_splits=7, shuffle=True, random_state=64)\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "best_model_info = {}\n",
    "\n",
    "for train_idx, test_idx in kfold.split(X_selected, y_labels):\n",
    "  X_train, X_test = X_selected[train_idx], X_selected[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "\n",
    "model = build_model(X_train.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=0.2, verbose=1)\n",
    "\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "if val_loss < best_val_loss:\n",
    "  best_val_loss = val_loss\n",
    "best_model = model\n",
    "best_model_info = {\n",
    "  'Feature_Count': feature_count,\n",
    "  'Validation_Loss': best_val_loss,\n",
    "  'Validation_Accuracy': history.history['val_accuracy'][-1],\n",
    "  'Neural_Architecture': [2600, 400, 40]\n",
    "}\n",
    "\n",
    "# Save model\n",
    "model_save_path = f\"C:/best_model_feature_{feature_count}.keras\"\n",
    "best_model.save(model_save_path)\n",
    "\n",
    "# Save features\n",
    "selected_features = X_full.columns[np.argsort(importances)[-feature_count:]]\n",
    "features_save_path = f\"C:/selected_features_{feature_count}.csv\"\n",
    "selected_features.to_series().to_csv(features_save_path, index=False, header=False)\n",
    "\n",
    "return best_model, best_model_info, X_test, y_test\n",
    "\n",
    "# Feature indices\n",
    "indices_5000 = np.argsort(importances)[-5000:]\n",
    "indices_1000 = np.argsort(importances)[-1000:]\n",
    "indices_2000 = np.argsort(importances)[-2000:]\n",
    "indices_3000 = np.argsort(importances)[-3000:]\n",
    "indices_4000 = np.argsort(importances)[-4000:]\n",
    "\n",
    "# Feature matrices\n",
    "X_selected_5000 = X_scaled[:, indices_5000]\n",
    "X_selected_1000 = X_scaled[:, indices_1000]\n",
    "X_selected_2000 = X_scaled[:, indices_2000]\n",
    "X_selected_3000 = X_scaled[:, indices_3000]\n",
    "X_selected_4000 = X_scaled[:, indices_4000]\n",
    "\n",
    "# Train, evaluate, and save models and features\n",
    "best_model_5000, best_model_info_5000, X_test_5000, y_test_5000 = train_and_evaluate(X_selected_5000, y_labels, y, 5000, X)\n",
    "best_model_1000, best_model_info_1000, X_test_1000, y_test_1000 = train_and_evaluate(X_selected_1000, y_labels, y, 1000, X)\n",
    "best_model_2000, best_model_info_2000, X_test_2000, y_test_2000 = train_and_evaluate(X_selected_2000, y_labels, y, 2000, X)\n",
    "best_model_3000, best_model_info_3000, X_test_3000, y_test_3000 = train_and_evaluate(X_selected_3000, y_labels, y, 3000, X)\n",
    "best_model_4000, best_model_info_4000, X_test_4000, y_test_4000 = train_and_evaluate(X_selected_4000, y_labels, y, 4000, X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
